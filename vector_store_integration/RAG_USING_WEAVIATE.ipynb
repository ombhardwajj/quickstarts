{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAztxTpIBiMF"
      },
      "outputs": [],
      "source": [
        "# Add virtual environment support if needed\n",
        "!apt-get install -qq python3.10-venv\n",
        "\n",
        "# Install required packages\n",
        "%pip install --quiet openai langchain-openai tiktoken pandas weaviate-client langchain-weaviate langchain-community\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDqAdjZTCbeB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"WEAVIATE_URL\"] = \"YOUR_WEAVIATE_URL\"\n",
        "os.environ[\"WEAVIATE_API_KEY\"] = \"YOUR_WEAVIATE_API_KEY\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnzOsbFaCepg"
      },
      "outputs": [],
      "source": [
        "import weaviate\n",
        "from weaviate.auth import AuthApiKey\n",
        "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "import pandas as pd\n",
        "\n",
        "# Connect to Weaviate with API key\n",
        "auth_config = AuthApiKey(api_key=os.getenv(\"WEAVIATE_API_KEY\"))\n",
        "\n",
        "try:\n",
        "    weaviate_client = weaviate.Client(\n",
        "        url=os.getenv(\"WEAVIATE_URL\"),\n",
        "        auth_client_secret=auth_config,\n",
        "    )\n",
        "    print(\"Successfully connected to Weaviate\", flush=True)\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to Weaviate: {e}\", flush=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SpJVDx4D23z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from typing import List\n",
        "\n",
        "# Initialize OpenAI client for embeddings\n",
        "openai_embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Convert user's query into a vector array to prep for similarity search\n",
        "def get_embedding_from_openai(query) -> List[float]:\n",
        "    return openai_embeddings.embed_query(query)\n",
        "\n",
        "# Use Weaviate to find matching chunks\n",
        "collection=\"Lead\"\n",
        "property=\"name\"\n",
        "def get_similar_chunks_from_weaviate(query: str) -> List[str]:\n",
        "    try:\n",
        "        embedding = get_embedding_from_openai(query)\n",
        "        near_vector = {\n",
        "            \"vector\": embedding\n",
        "        }\n",
        "        result = weaviate_client.query.get(collection, [property]).with_near_vector(near_vector).do()\n",
        "\n",
        "        if 'data' in result and 'Get' in result['data'] and collection in result['data']['Get']:\n",
        "            chunks = [res[property] for res in result['data']['Get'][collection]]\n",
        "            return chunks\n",
        "        else:\n",
        "            print(\"Unexpected result format:\", result, flush=True)\n",
        "            return []\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Weaviate query: {e}\", flush=True)\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qu1C3e1TE2YL"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from openai import OpenAI  \n",
        "\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Use OpenAI to complete the response\n",
        "def get_completion_from_openai(question, document_chunks: List[str], model_name=\"gpt-3.5-turbo\"):\n",
        "    chunks = \"\\n\\n\".join(document_chunks)\n",
        "\n",
        "    try:\n",
        "        completion = client.chat.completions.create(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are an assistant. Answer the question based on the context. Do not use any other information. Be concise.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Context:\\n{chunks}\\n\\n{question}\\n\\nAnswer:\"}\n",
        "            ],\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return completion.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during OpenAI completion: {e}\", flush=True)\n",
        "        return \"There was an error generating the response.\"\n",
        "\n",
        "\n",
        "# Putting it all together\n",
        "def get_response(query, model_name=\"gpt-3.5-turbo\"):\n",
        "    chunks = get_similar_chunks_from_weaviate(query)\n",
        "    if len(chunks) == 0:\n",
        "        return \"I am sorry, I do not have the context to answer your question.\"\n",
        "    else:\n",
        "        return get_completion_from_openai(query, chunks, model_name)\n",
        "\n",
        "# Ask a question\n",
        "query = 'How many lead work in BNY?'\n",
        "response = get_response(query)\n",
        "\n",
        "print(f\"\\n\\nResponse from LLM:\\n\\n{response}\", flush=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
